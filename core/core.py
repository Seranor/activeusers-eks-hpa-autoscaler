import sys
import os
from datetime import datetime
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from lib.logger import app_logger as logger
from lib.get_analytics_user import get_active_users
from lib.get_analytics_user import get_mock_users
from lib.query_data import ScalingConfigManager
from lib.feishu_bot import FeishuRichTextBot
from lib.aws_db import AWSDBManager
from lib.aws_eks import EKSManager
from lib.k8s_client import K8sClient
from conf import settings


class AutoScalingService:
    def __init__(self):
        """åˆå§‹åŒ–è‡ªåŠ¨ä¼¸ç¼©æœåŠ¡"""
        self.scaling_manager = ScalingConfigManager()
        self.feishu_bot = FeishuRichTextBot(
            webhook_url=settings.FEISHU_WEBHOOK_URL,
            max_retries=5,
            retry_delay=1
        )
        self.aws_db_manager = AWSDBManager(
            access_key_id=settings.AWS_ACCESS_KEY_ID,
            secret_access_key=settings.AWS_SECRET_ACCESS_KEY,
            region_name=settings.AWS_REGION
        )
        self.k8s_client = K8sClient(
            kube_config_path=settings.KUBE_FILE_PATH,
            context_name=settings.CLUSTER_CONTEXT
        )
        self.aws_eks_manager = EKSManager(
            settings.AWS_REGION, 
            settings.AWS_ACCESS_KEY_ID, 
            settings.AWS_SECRET_ACCESS_KEY, 
            settings.EKS_CLUSTER_NAME
        )

        # è®°å½•ä¸Šæ¬¡æ‰©å®¹äº‹ä»¶
        self.last_scaling_time = None
        self.last_level = None
        
        logger.info("è‡ªåŠ¨ä¼¸ç¼©æœåŠ¡å·²åˆå§‹åŒ–")
    
    def get_current_capacity_level(self):
        """
        æ ¹æ®çº¿ä¸Šçš„DBé…ç½®ä»¥åŠHPAæœ€å°é…ç½® è·å–åˆ°å½“å‰é…ç½®çº§åˆ«
        """
        try:
            # è·å–DB é…ç½®ç±»å‹
            rds_instance_types = self.aws_db_manager.get_rds_cluster_instance_type(settings.RDS_CLUSTER_NAME)
            if rds_instance_types:
                rds_highest_id, rds_highest_type = get_highest_instance_config(rds_instance_types)
                logger.info(f"æœ€é«˜é…ç½®å®ä¾‹: {rds_highest_id}: {rds_highest_type}")
            else:
                logger.error(f"RDSé›†ç¾¤ {settings.RDS_CLUSTER_NAME} å®ä¾‹ç±»å‹æŸ¥è¯¢å¤±è´¥")

            # è·å–Redisç±»å‹
            redis_node_type = self.aws_db_manager.get_elasticache_redis_node_type(settings.REDIS_OSS_NAME)
            if redis_node_type:
                logger.info(f"Rediså®ä¾‹ {settings.REDIS_OSS_NAME} å½“å‰èŠ‚ç‚¹ç±»å‹: {redis_node_type}")
            else:
                logger.error(f"Rediså®ä¾‹ {settings.REDIS_OSS_NAME} èŠ‚ç‚¹ç±»å‹æŸ¥è¯¢å¤±è´¥")

            # ç›´æ¥é€šè¿‡ istioçš„replic rds redis è·å–å½“å‰çº§åˆ«
            # å½“å‰istio-ingress hpa min
            istio_ingress_hpa = self.k8s_client.get_hpa_scaling_config(
                hpa_name=settings.HPA_NAME,
                namespace=settings.HPA_NAMESPACE
            )
            istio_ingress_hpa_min = istio_ingress_hpa['min_replicas']

            # æ¯”å¯¹é…ç½® è¿”å›å½“å‰æ•°æ®åº“ä¸­è®°å½•çš„ level 
            capacity_level = self.scaling_manager.determine_capacity_level(
                hpa_name=settings.HPA_NAME, 
                namespace=settings.HPA_NAMESPACE, 
                service_name=settings.HPA_SERVICE_NAME,
                redis_instance_type=redis_node_type, 
                postgres_instance_type=rds_highest_type,
                replicas=istio_ingress_hpa_min
            )
            return capacity_level.user_capacity

        except Exception as e:
            logger.error(f"è·å–å½“å‰å®¹é‡çº§åˆ«æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return 0

    def check_and_scale(self):
        """æ£€æŸ¥ç”¨æˆ·æ•°é‡å¹¶æ‰§è¡Œä¼¸ç¼©æ“ä½œ"""
        try:
            # è·å–å½“å‰æ´»è·ƒç”¨æˆ·æ•°
            user_count = get_active_users(settings.KEY_FILE_LOCATION, settings.PROPERTY_ID)
            # user_count = get_mock_users(api_url="http://10.4.59.123:5000/api/online-users")
            # user_count = 1000
            
            # åˆ¤æ–­æ˜¯å¦éœ€è¦æ‰©å®¹åŠè·å–ç›®æ ‡çº§åˆ«
            scaling_required, target_level, current_capacity = self._evaluate_scaling_need(user_count)
            if not scaling_required:
                return
            
            # è·å–å®Œæ•´é…ç½®
            complete_config = self.scaling_manager.get_complete_config(user_count)
            
            # æ£€æŸ¥åŸºç¡€è®¾æ–½æ˜¯å¦æ»¡è¶³è¦æ±‚
            infrastructure_ready, db_status, redis_status = self._check_infrastructure(complete_config)
            
            # å‡†å¤‡å¹¶å‘é€é€šçŸ¥
            self._send_scaling_notification(
                user_count, current_capacity, target_level.user_capacity, 
                infrastructure_ready, db_status, redis_status, complete_config
            )
            
            # å¦‚æœåŸºç¡€è®¾æ–½å·²å‡†å¤‡å¥½ï¼Œæ‰§è¡ŒK8sèµ„æºä¼¸ç¼©
            if infrastructure_ready:
                # åˆ¤æ–­èŠ‚ç‚¹ç»„æƒ…å†µï¼Œå¦‚æœæœŸæœ›å€¼ä¸º0 éœ€è¦ä¿®æ”¹ï¼Œå¦‚æœä¸ä¸º0 ä¸ç”¨å¤„ç†ï¼Œç›´æ¥å‡çº§
                nodegroups = self.aws_eks_manager.list_nodegroups()
                node_info = {}
                for nodegroup_name in nodegroups:
                    desired_size = self.aws_eks_manager.get_nodegroup_desired_size(nodegroup_name)
                    node_info[nodegroup_name] = desired_size
                for pool in node_info:
                    if node_info[pool] == 0:
                        up_pool_state = self.aws_eks_manager.update_nodegroup_scaling(pool,0,20,1)

                scaling_results = self._scale_kubernetes_resources(complete_config)
                if scaling_results:
                    scaling_message = []
                    for res in scaling_results:
                        scaling_message.append(
                            [
                                {"tag": "text", "text": str(res)},
                            ]
                        )
                self.feishu_bot.send_rich_text(
                    title = f"âš ï¸ èµ„æºä¼¸ç¼©ä¿¡æ¯",
                    content = scaling_message
                )
                self._update_scaling_history(target_level.user_capacity)
                
        except Exception as e:
            error_message = [
                [
                    {"tag": "text", "text": "é”™è¯¯ä¿¡æ¯"},
                    {"tag": "text", "text": {str(e)}},
                ]
            ]
            logger.error(error_message, exc_info=True)
            # å‘é€é”™è¯¯é€šçŸ¥
            self.feishu_bot.send_rich_text(
                title="âŒ è‡ªåŠ¨ä¼¸ç¼©æœåŠ¡å¼‚å¸¸",
                content=error_message
            )

    def _evaluate_scaling_need(self, user_count):
        """è¯„ä¼°æ˜¯å¦éœ€è¦è¿›è¡Œæ‰©å®¹"""
        # è·å–ç›®æ ‡å®¹é‡çº§åˆ«
        target_level = self.scaling_manager.get_target_level(user_count)
        if not target_level:
            logger.error("æ— æ³•ç¡®å®šç›®æ ‡å®¹é‡çº§åˆ«")
            return False, None, None
        
        level_user_capacity = target_level.user_capacity

        logger.info(f"ç›®æ ‡å®¹é‡çº§åˆ«: {level_user_capacity}")
        
        # è·å–å½“å‰ç³»ç»Ÿé…ç½®çš„å®¹é‡çº§åˆ«
        current_capacity = self.get_current_capacity_level()
        

        # å¦‚æœç”¨æˆ·æ•°ä½äº600 ä¸éœ€è¦æ“ä½œ
        if user_count < 600:
            # è¿˜æ˜¯éœ€è¦æ£€æŸ¥å½“å‰é…ç½®æ˜¯å¦é«˜äº600ï¼Œé«˜äº600 è¿˜æ˜¯éœ€è¦é™é…
            if current_capacity > 600:
                logger.info(f"ç”¨æˆ·æ•°ä½äº600ï¼Œä½†å½“å‰é…ç½®å®¹é‡çº§åˆ«ä¸º{current_capacity}ï¼Œéœ€è¦é™é…")
                
                # å‘é€é™çº§é€šçŸ¥ä½†ä¸æ‰§è¡Œæ“ä½œ
                message_content = [
                    [                
                        {"tag": "text", "text": "æ—¶é—´: "},
                        {"tag": "text", "text": datetime.now().strftime('%Y-%m-%d %H:%M:%S')},
                    ],
                    [
                        {"tag": "text", "text": "ğŸ‘¥ å½“å‰æ´»è·ƒç”¨æˆ·æ•°: "},
                        {"tag": "text", "text": str(user_count)},
                    ],
                    [
                        {"tag": "text", "text": "âš™ï¸ å½“å‰ç³»ç»Ÿé…ç½®å®¹çº³çº§åˆ«: "},
                        {"tag": "text", "text": str(current_capacity)},
                    ],
                    [
                        {"tag": "text", "text": "âš™ï¸ éœ€è¦è°ƒæ•´ç›®æ ‡é…ç½®å®¹çº³çº§åˆ«: "},
                        {"tag": "text", "text": str(level_user_capacity)},
                    ],
                    [
                        {"tag": "text", "text": "æ ¹æ®é…ç½®ï¼Œéœ€ä¸Šçº¿æ‰‹åŠ¨è°ƒæ•´é™é…"},
                    ]
                ]
                self.feishu_bot.send_rich_text(
                    title = f"âš ï¸ é™é…é€šçŸ¥ - ç”¨æˆ·æ•°ä½äº600",
                    content = message_content
                )
            else:
                logger.info("ç”¨æˆ·æ•°ä½äº600ï¼Œå½“å‰é…ç½®é€‚åˆï¼Œä¸éœ€è¦æ“ä½œ")

            return False, None, None
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦é™çº§ - å¦‚æœç›®æ ‡å®¹é‡å°äºå½“å‰å®¹é‡ï¼Œåªå‘é€é€šçŸ¥ä¸æ‰§è¡Œæ“ä½œ
        is_downgrade = level_user_capacity < current_capacity
        if is_downgrade:
            logger.info(f"æ£€æµ‹åˆ°é™çº§è¯·æ±‚ï¼ˆå½“å‰:{current_capacity} -> ç›®æ ‡:{level_user_capacity}ï¼‰ï¼Œä»…å‘é€é€šçŸ¥ä¸æ‰§è¡Œæ“ä½œ")
            
            # å‘é€é™çº§é€šçŸ¥ä½†ä¸æ‰§è¡Œæ“ä½œ
            message_content = [
                [                
                    {"tag": "text", "text": "æ—¶é—´: "},
                    {"tag": "text", "text": datetime.now().strftime('%Y-%m-%d %H:%M:%S')},
                ],
                [
                    {"tag": "text", "text": "ğŸ‘¥ å½“å‰æ´»è·ƒç”¨æˆ·æ•°: "},
                    {"tag": "text", "text": str(user_count)},
                ],
                [
                    {"tag": "text", "text": "âš™ï¸ å½“å‰ç³»ç»Ÿé…ç½®å®¹çº³çº§åˆ«: "},
                    {"tag": "text", "text": str(current_capacity)},
                ],
                [
                    {"tag": "text", "text": "âš™ï¸ éœ€è¦è°ƒæ•´ç›®æ ‡é…ç½®å®¹çº³çº§åˆ«: "},
                    {"tag": "text", "text": str(level_user_capacity)},
                ],
                [
                    {"tag": "text", "text": "æ ¹æ®é…ç½®ï¼Œéœ€ä¸Šçº¿æ‰‹åŠ¨è°ƒæ•´é™é…"},
                ]
            ]
            self.feishu_bot.send_rich_text(
                title = f"âš ï¸ é™é…é€šçŸ¥",
                content = message_content
            )
            return False, None, None
        
        # å¦‚æœç›®æ ‡çº§åˆ«ä¸å½“å‰çº§åˆ«ç›¸åŒï¼Œä¸æ‰§è¡Œæ“ä½œ
        if level_user_capacity == current_capacity:
            logger.info(f"ç›®æ ‡å®¹é‡çº§åˆ«ä¸å½“å‰é…ç½®ç›¸åŒ ({level_user_capacity})ï¼Œæ— éœ€æ“ä½œ")
            return False, None, None
        
        # æ£€æŸ¥æ˜¯å¦ä¸ä¸Šæ¬¡æ‰©å®¹çº§åˆ«ç›¸åŒä¸”æ—¶é—´é—´éš”å°äº10åˆ†é’Ÿ
        current_time = datetime.now()
        if (self.last_scaling_time and self.last_level == level_user_capacity and 
            (current_time - self.last_scaling_time).seconds < 600):
            logger.info(f"æœ€è¿‘å·²æ‰§è¡Œè¿‡çº§åˆ« {level_user_capacity} çš„æ‰©å®¹ï¼Œè·³è¿‡")
            return False, None, None
        
        return True, target_level, current_capacity

    def _check_infrastructure(self, complete_config):
        """
        æ£€æŸ¥æ•°æ®åº“å’ŒRedisé…ç½®æ˜¯å¦æ»¡è¶³ç›®æ ‡é…ç½®ï¼Œä»…æ¯”è¾ƒå®ä¾‹ç±»å‹ 
        å¦‚æœç›®æ ‡é…ç½®æ¯”å½“å‰é…ç½®å¤§çš„å¤šä¹Ÿæ˜¯å¯ä»¥å‡çº§
        capacity_level æ¯”å¯¹
        """
        try:
            # è·å–å½“å‰RDSå®ä¾‹ç±»å‹
            rds_instance_type = self.aws_db_manager.get_rds_cluster_instance_type(settings.RDS_CLUSTER_NAME)
            if not rds_instance_type:
                logger.error(f"è·å–RDSå®ä¾‹ç±»å‹å¤±è´¥: {settings.RDS_CLUSTER_NAME}")
                return False, None, None
            

            # è·å–æœ€é«˜é…ç½®çš„RDSèŠ‚ç‚¹ç±»å‹
            _, highest_rds_type = get_highest_instance_config(rds_instance_type)

            # è·å–å½“å‰Rediså®ä¾‹ç±»å‹
            redis_type = self.aws_db_manager.get_elasticache_redis_node_type(settings.REDIS_OSS_NAME)
            if not redis_type:
                logger.error(f"è·å–Rediså®ä¾‹ç±»å‹å¤±è´¥: {settings.REDIS_OSS_NAME}")
                return False, None, None
            
            # è·å–ç›®æ ‡å®ä¾‹ç±»å‹
            target_db_type = complete_config["postgres"]['instance_type']
            target_redis_type = complete_config["redis"]['instance_type']
            
            # ä¸èƒ½ç®€å•çš„åˆ¤æ–­å½“å‰é…ç½®ç›¸ç­‰ï¼Œéœ€è¦åˆ¤æ–­ å½“å‰é…ç½®å®¹é‡äººæ•°æ¯”ç›®æ ‡é…ç½®å°(éœ€è¦é€šçŸ¥å‡çº§æ•°æ®åº“)ã€‚å½“å‰é…ç½®å®¹é‡äººæ•°æ¯”ç›®æ ‡é…ç½®å¤§ï¼Œå½“å‰é…ç½®å®¹é‡äººæ•°å’Œç›®æ ‡é…ç½®ç›¸ç­‰ ï¼ˆè¿”å›True ä¸ç”¨é€šçŸ¥å‡çº§ï¼‰
            is_ready = False
            # å½“å‰é…ç½®çº§åˆ«å®¹é‡äººæ•°
            current_db_level = self.scaling_manager.get_user_capacity_by_postgres_instance_type(highest_rds_type)
            current_redis_level = self.scaling_manager.get_user_capacity_by_redis_instance_type(redis_type)

            # ç›®æ ‡é…ç½®çº§åˆ«å®¹é‡äººæ•°
            target_db_level = complete_config["postgres"]['level']
            target_redis_level = complete_config["redis"]['level']

            # åŸºäºå®¹é‡äººæ•°åˆ¤æ–­æ˜¯å¦æ»¡è¶³è¦æ±‚
            db_meets_capacity = False
            redis_meets_capacity = False

            # åˆ¤æ–­æ•°æ®åº“å®¹é‡æ˜¯å¦æ»¡è¶³è¦æ±‚
            if current_db_level is not None and target_db_level is not None:
                db_meets_capacity = current_db_level >= target_db_level
                logger.info(f"æ•°æ®åº“å®¹é‡æ¯”è¾ƒ: å½“å‰å®¹é‡ {current_db_level} {'â‰¥' if db_meets_capacity else '<'} ç›®æ ‡å®¹é‡ {target_db_level}")
            else:
                logger.warning(f"æ— æ³•æ¯”è¾ƒæ•°æ®åº“å®¹é‡: å½“å‰å®¹é‡ {current_db_level}, ç›®æ ‡å®¹é‡ {target_db_level}")

            # åˆ¤æ–­Rediså®¹é‡æ˜¯å¦æ»¡è¶³è¦æ±‚
            if current_redis_level is not None and target_redis_level is not None:
                redis_meets_capacity = current_redis_level >= target_redis_level
                logger.info(f"Rediså®¹é‡æ¯”è¾ƒ: å½“å‰å®¹é‡ {current_redis_level} {'â‰¥' if redis_meets_capacity else '<'} ç›®æ ‡å®¹é‡ {target_redis_level}")
            else:
                logger.warning(f"æ— æ³•æ¯”è¾ƒRediså®¹é‡: å½“å‰å®¹é‡ {current_redis_level}, ç›®æ ‡å®¹é‡ {target_redis_level}")

            # æ„å»ºè¿”å›çŠ¶æ€ä¿¡æ¯
            db_status = {
                "current": {
                    "type": highest_rds_type,
                    "capacity": current_db_level
                },
                "target": {
                    "type": target_db_type,
                    "capacity": target_db_level
                },
                "meets_capacity": db_meets_capacity
            }

            redis_status = {
                "current": {
                    "type": redis_type,
                    "capacity": current_redis_level
                },
                "target": {
                    "type": target_redis_type,
                    "capacity": target_redis_level
                },
                "meets_capacity": redis_meets_capacity
            }

            # åªæœ‰å½“ä¸¤è€…éƒ½æ»¡è¶³å®¹é‡è¦æ±‚æ—¶ï¼Œæ‰è¿”å›True
            is_ready = db_meets_capacity and redis_meets_capacity
            # is_ready = True

            # è¿”å›æ•´ä½“æ»¡è¶³çŠ¶æ€å’Œå„ç»„ä»¶çŠ¶æ€
            return is_ready, db_status, redis_status

            
        except Exception as e:
            logger.error(f"æ£€æŸ¥åŸºç¡€è®¾æ–½æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return False, None, None

    def _scale_kubernetes_resources(self, complete_config):
        """æ‰§è¡ŒKubernetesèµ„æºçš„ä¼¸ç¼©æ“ä½œ"""
        scaling_results = []
        
        # éå†æ‰€æœ‰æœåŠ¡å¹¶è¿›è¡Œæ›´æ–°
        for namespace, services in complete_config["services"].items():
            for service_name, service_config in services.items():
                # æ›´æ–°HPAæœ€å°å‰¯æœ¬æ•°
                if service_config["hpa_name"]:
                    try:
                        hpa_name = service_config["hpa_name"]
                        replicas = service_config["replicas"]
                        
                        # è°ƒç”¨K8så®¢æˆ·ç«¯æ›´æ–°HPA
                        self.k8s_client.update_hpa_scaling(
                            namespace=namespace,
                            hpa_name=hpa_name,
                            min_replicas=replicas
                        )
                        
                        scaling_results.append(
                            f"- âœ… å·²å°† {namespace}/{hpa_name} æœ€å°å‰¯æœ¬æ•°æ›´æ–°ä¸º {replicas}"
                        )
                        logger.info(f"å·²å°† {namespace}/{hpa_name} æœ€å°å‰¯æœ¬æ•°æ›´æ–°ä¸º {replicas}")
                    except Exception as e:
                        error_msg = f"- âŒ æ›´æ–° {namespace}/{hpa_name} å¤±è´¥: {str(e)}"
                        scaling_results.append(error_msg)
                        logger.error(error_msg)
                
                # æ›´æ–°èŠ‚ç‚¹äº²å’Œæ€§
                if service_config["pool_name"]:
                    try:
                        # è°ƒç”¨K8så®¢æˆ·ç«¯æ›´æ–°èŠ‚ç‚¹äº²å’Œæ€§
                        if  service_config["pool_name"]:
                            self.k8s_client.set_nodegroup_affinity(
                                namespace=namespace,
                                deployment_name=service_name,
                                nodegroup_key="eks.amazonaws.com/nodegroup",
                                nodegroup_values=service_config["pool_name"]
                            )
                            scaling_results.append(
                                f"- âœ… å·²å°† {namespace}/{service_name} èŠ‚ç‚¹äº²å’Œæ€§æ›´æ–°ä¸º {service_config['pool_name']}"
                            )
                            logger.info(f"å·²å°† {namespace}/{service_name} èŠ‚ç‚¹äº²å’Œæ€§æ›´æ–°ä¸º {service_config['pool_name']}")
                    except Exception as e:
                        error_msg = f"- âŒ æ›´æ–° {namespace}/{service_name} èŠ‚ç‚¹äº²å’Œæ€§å¤±è´¥: {str(e)}"
                        scaling_results.append(error_msg)
                        logger.error(error_msg)
        
        return scaling_results

    def _send_scaling_notification(self, user_count, current_capacity, target_capacity, 
                                infrastructure_ready, db_status, redis_status, complete_config):
        """å‘é€æ‰©å®¹é€šçŸ¥ï¼Œä½¿ç”¨é£ä¹¦å¯Œæ–‡æœ¬æ ¼å¼"""
        message_title = f"ğŸš¨ [ç”Ÿäº§ç¯å¢ƒ] ç³»ç»Ÿèµ„æºé€šçŸ¥"
        
        # æ„å»ºç¬¦åˆé£ä¹¦å¯Œæ–‡æœ¬æ ¼å¼çš„æ¶ˆæ¯å†…å®¹
        message_content = [
            # æ£€æŸ¥æ—¶é—´
            [
                {"tag": "text", "text": "æ—¶é—´: "},
                {"tag": "text", "text": datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
            ],
            # å½“å‰ç”¨æˆ·æ•°
            [
                {"tag": "text", "text": "ğŸ‘¥ å½“å‰æ´»è·ƒç”¨æˆ·æ•°: "},
                {"tag": "text", "text": str(user_count)}
            ],
            # å½“å‰é…ç½®çº§åˆ«
            [
                {"tag": "text", "text": "âš™ï¸ å½“å‰ç³»ç»Ÿé…ç½®å®¹çº³çº§åˆ«: "},
                {"tag": "text", "text": str(current_capacity)}
            ],
            # ç›®æ ‡é…ç½®çº§åˆ«
            [
                {"tag": "text", "text": "âš™ï¸ éœ€è¦è°ƒæ•´ç›®æ ‡é…ç½®å®¹çº³çº§åˆ«: "},
                {"tag": "text", "text": str(target_capacity)}
            ]
        ]
        
        if infrastructure_ready:
            # æ·»åŠ åŸºç¡€è®¾æ–½å°±ç»ªä¿¡æ¯
            message_content.append([
                {"tag": "text", "text": "ğŸ”§ åŸºç¡€è®¾æ–½çŠ¶æ€: "},
                {"tag": "text", "text": "âœ… æ•°æ®åº“å’ŒRedisé…ç½®å·²æ»¡è¶³è¦æ±‚"}
            ])
            message_content.append([
                {"tag": "text", "text": "RDSå½“å‰é…ç½®: "},
                {"tag": "text", "text": str(db_status['current']['type'])}
            ])
            message_content.append([
                {"tag": "text", "text": "Rediså½“å‰é…ç½®: "},
                {"tag": "text", "text": str(redis_status['current']['type'])}
            ])
        
            # K8sæ‰©å®¹ä¿¡æ¯å ä½ç¬¦
            message_content.append([
                {"tag": "text", "text": "ğŸ”„ K8sèµ„æºHPAæ‰©å®¹å°†è¢«æ‰§è¡Œ"}
            ])
        else:
            # æ·»åŠ åŸºç¡€è®¾æ–½ä¸æ»¡è¶³è¦æ±‚çš„ä¿¡æ¯
            message_content.append([
                {"tag": "text", "text": "ğŸ”§ åŸºç¡€è®¾æ–½çŠ¶æ€: "},
                {"tag": "text", "text": "âŒ DBèµ„æºç±»å‹ä¸æ»¡è¶³è¦æ±‚"}
            ])
            message_content.append([])
            # æ·»åŠ PostgreSQLé…ç½®å·®å¼‚
            if not db_status["meets_capacity"]:
                message_content.append([
                    {"tag": "text", "text": "ğŸ“Œ èµ„æºå‡çº§å»ºè®®ï¼š"},
                ])
                message_content.append([
                    {"tag": "text", "text": "ğŸ—„ï¸ PostgreSQL æ•°æ®åº“"}
                ])
                message_content.append([
                    {"tag": "text", "text": "å½“å‰é…ç½®: "},
                    {"tag": "text", "text": str(db_status['current']['type'])}
                ])
                message_content.append([
                    {"tag": "text", "text": "éœ€è¦é…ç½®: "},
                    {"tag": "text", "text": str(db_status['target']['type'])}
                ])
            message_content.append([])
            # æ·»åŠ Redisé…ç½®å·®å¼‚
            if not redis_status["meets_capacity"]:
                message_content.append([
                    {"tag": "text", "text": "ğŸ§  Redis ç¼“å­˜"}
                ])
                message_content.append([
                    {"tag": "text", "text": "å½“å‰é…ç½®: "},
                    {"tag": "text", "text": str(redis_status['current']['type'])}
                ])
                message_content.append([
                    {"tag": "text", "text": "éœ€è¦é…ç½®: "},
                    {"tag": "text", "text": str(redis_status['target']['type'])}
                ])
            
            # æ·»åŠ æ“ä½œå»ºè®®
            message_content.append([
                {"tag": "text", "text": "âš ï¸ è¯·è¿ç»´äººå‘˜å°½å¿«å‡çº§æ•°æ®åº“å’Œç¼“å­˜èµ„æº!"}
            ])
        
        # å‘é€é€šçŸ¥
        self.feishu_bot.send_rich_text(
            title=message_title,
            content=message_content
        )
        logger.info(f"å·²å‘é€æ‰©å®¹é€šçŸ¥: {message_title}")

    def _update_scaling_history(self, target_capacity):
        """æ›´æ–°æ‰©å®¹å†å²è®°å½•"""
        self.last_scaling_time = datetime.now()
        self.last_level = target_capacity

def get_highest_instance_config(instance_types):
    """
    è·å–å®ä¾‹åˆ—è¡¨ä¸­é…ç½®æœ€é«˜çš„å®ä¾‹
    
    Args:
        instance_types (dict): åŒ…å«å®ä¾‹IDå’Œå®ä¾‹ç±»å‹çš„å­—å…¸ {instance_id: instance_type}
        
    Returns:
        tuple: (highest_instance_id, highest_instance_type) é…ç½®æœ€é«˜çš„å®ä¾‹IDå’Œç±»å‹
    """
    if not instance_types:
        return None, None
        
    highest_id = None
    highest_type = None
    max_value = -1
    
    for instance_id, instance_type in instance_types.items():
        # logger.info(f"å®ä¾‹ {instance_id}: {instance_type}")
        size = instance_type.split('.')[-1]  # è·å–æœ€åä¸€éƒ¨åˆ†ï¼Œå¦‚large, xlarge, 8xlarge
        
        # è®¡ç®—å®ä¾‹å¤§å°çš„æ•°å€¼
        if size == "large":
            value = 1
        elif size == "xlarge":
            value = 2
        elif "xlarge" in size:
            # å¤„ç†å¦‚2xlarge, 4xlargeç­‰æƒ…å†µ
            value = int(size.replace("xlarge", "") or "0") + 2
        else:
            # å¤„ç†å…¶ä»–å¯èƒ½çš„æƒ…å†µ
            value = 0
            
        if value > max_value:
            max_value = value
            highest_id = instance_id
            highest_type = instance_type
            
    return highest_id, highest_type

if __name__ == "__main__":
    auto_scaling = AutoScalingService()
    level = auto_scaling.get_current_capacity_level()
    print(f"å½“å‰çº§åˆ«:{level}")

    # auto_scaling.check_and_scale()